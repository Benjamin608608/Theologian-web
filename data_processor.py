#!/usr/bin/env python3
"""
ç¥å­¸æ–‡æª”æ•¸æ“šè™•ç†è…³æœ¬
ç”¨æ–¼å°‡2GBçš„ç¥å­¸æ–‡æª”è½‰æ›ç‚ºFAISSå‘é‡ç´¢å¼•

ä½¿ç”¨æ–¹æ³•:
python data_processor.py --input_dir /path/to/documents --output_dir ./processed_data
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
import hashlib
from datetime import datetime

import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from tqdm import tqdm
import psutil

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TheologyDocumentProcessor:
    """ç¥å­¸æ–‡æª”è™•ç†å™¨"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        self.model_name = model_name
        self.model = None
        self.chunk_size = 512
        self.overlap = 50
        
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info(f"è¼‰å…¥æ¨¡å‹: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def load_documents_from_directory(self, input_dir: str) -> List[Dict[str, Any]]:
        """å¾ç›®éŒ„è¼‰å…¥æ‰€æœ‰æ–‡æª”"""
        logger.info(f"é–‹å§‹è™•ç†ç›®éŒ„: {input_dir}")
        
        documents = []
        input_path = Path(input_dir)
        
        if not input_path.exists():
            raise ValueError(f"è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
        
        # æ”¯æ´çš„æ–‡ä»¶æ ¼å¼
        supported_formats = {
            '.txt': self.load_text_file,
            '.md': self.load_text_file,
            '.json': self.load_json_file,
            '.csv': self.load_csv_file,
            '.tsv': self.load_tsv_file,
        }
        
        # éæ­¸è™•ç†æ‰€æœ‰æ–‡ä»¶
        total_files = 0
        processed_files = 0
        
        for file_path in input_path.rglob('*'):
            if file_path.is_file():
                total_files += 1
                
                file_extension = file_path.suffix.lower()
                if file_extension in supported_formats:
                    try:
                        loader_func = supported_formats[file_extension]
                        file_documents = loader_func(file_path)
                        documents.extend(file_documents)
                        processed_files += 1
                        
                        if processed_files % 100 == 0:
                            logger.info(f"å·²è™•ç† {processed_files}/{total_files} å€‹æ–‡ä»¶")
                            
                    except Exception as e:
                        logger.error(f"è™•ç†æ–‡ä»¶å¤±æ•— {file_path}: {e}")
                else:
                    logger.debug(f"è·³éä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_path}")
        
        logger.info(f"æ–‡æª”è¼‰å…¥å®Œæˆ: {len(documents)} å€‹æ–‡æª”ç‰‡æ®µ")
        return documents
    
    def load_text_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """è¼‰å…¥æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            # å˜—è©¦å…¶ä»–ç·¨ç¢¼
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    content = f.read()
            except UnicodeDecodeError:
                with open(file_path, 'r', encoding='latin1') as f:
                    content = f.read()
        
        return self.chunk_document(content, str(file_path))
    
    def load_json_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """è¼‰å…¥JSONæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å¦‚æœæ˜¯å–®å€‹å°è±¡ï¼Œè½‰æ›ç‚ºåˆ—è¡¨
        if isinstance(data, dict):
            data = [data]
        
        documents = []
        for item in data:
            if isinstance(item, dict):
                # æå–æ–‡æœ¬å…§å®¹
                content = self.extract_text_from_dict(item)
                if content:
                    docs = self.chunk_document(content, str(file_path), item)
                    documents.extend(docs)
        
        return documents
    
    def load_csv_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """è¼‰å…¥CSVæ–‡ä»¶"""
        import pandas as pd
        
        df = pd.read_csv(file_path)
        documents = []
        
        for idx, row in df.iterrows():
            # åˆä½µæ‰€æœ‰åˆ—çš„å…§å®¹
            content = ' '.join([str(val) for val in row.values if pd.notna(val)])
            if content.strip():
                docs = self.chunk_document(content, f"{file_path}:row_{idx}", row.to_dict())
                documents.extend(docs)
        
        return documents
    
    def load_tsv_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """è¼‰å…¥TSVæ–‡ä»¶"""
        import pandas as pd
        
        df = pd.read_csv(file_path, sep='\t')
        documents = []
        
        for idx, row in df.iterrows():
            content = ' '.join([str(val) for val in row.values if pd.notna(val)])
            if content.strip():
                docs = self.chunk_document(content, f"{file_path}:row_{idx}", row.to_dict())
                documents.extend(docs)
        
        return documents
    
    def extract_text_from_dict(self, data: Dict[str, Any]) -> str:
        """å¾å­—å…¸ä¸­æå–æ–‡æœ¬å…§å®¹"""
        text_parts = []
        
        # å¸¸è¦‹çš„æ–‡æœ¬å­—æ®µå
        text_fields = ['text', 'content', 'body', 'description', 'title', 'name', 'summary']
        
        for field in text_fields:
            if field in data and isinstance(data[field], str):
                text_parts.append(data[field])
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¨™æº–å­—æ®µï¼Œæå–æ‰€æœ‰å­—ç¬¦ä¸²å€¼
        if not text_parts:
            for key, value in data.items():
                if isinstance(value, str) and len(value.strip()) > 10:
                    text_parts.append(f"{key}: {value}")
        
        return ' '.join(text_parts)
    
    def chunk_document(self, content: str, source: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """å°‡æ–‡æª”åˆ†å¡Š"""
        if not content or len(content.strip()) < 20:
            return []
        
        chunks = []
        content_length = len(content)
        
        for i in range(0, content_length, self.chunk_size - self.overlap):
            chunk_content = content[i:i + self.chunk_size].strip()
            
            # è·³éå¤ªçŸ­çš„ç‰‡æ®µ
            if len(chunk_content) < 50:
                continue
            
            chunk_id = hashlib.md5(f"{source}_{i}".encode()).hexdigest()
            
            chunk_metadata = {
                "source": source,
                "start_pos": i,
                "end_pos": min(i + self.chunk_size, content_length),
                "chunk_index": len(chunks),
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ åŸå§‹å…ƒæ•¸æ“š
            if metadata:
                chunk_metadata.update(metadata)
            
            chunk = {
                "id": chunk_id,
                "content": chunk_content,
                "metadata": chunk_metadata
            }
            
            chunks.append(chunk)
        
        return chunks
    
    def vectorize_documents(self, documents: List[Dict[str, Any]], batch_size: int = 64) -> np.ndarray:
        """å‘é‡åŒ–æ–‡æª”"""
        logger.info(f"é–‹å§‹å‘é‡åŒ– {len(documents)} å€‹æ–‡æª”ç‰‡æ®µ...")
        
        texts = [doc['content'] for doc in documents]
        all_vectors = []
        
        # åˆ†æ‰¹è™•ç†é¿å…å…§å­˜æº¢å‡º
        for i in tqdm(range(0, len(texts), batch_size), desc="å‘é‡åŒ–é€²åº¦"):
            batch_texts = texts[i:i + batch_size]
            batch_vectors = self.model.encode(
                batch_texts, 
                show_progress_bar=False,
                convert_to_numpy=True
            )
            all_vectors.append(batch_vectors)
            
            # å…§å­˜ç›£æ§
            memory_info = psutil.virtual_memory()
            if memory_info.percent > 85:
                logger.warning(f"å…§å­˜ä½¿ç”¨ç‡éé«˜: {memory_info.percent}%")
        
        vectors = np.vstack(all_vectors)
        logger.info(f"å‘é‡åŒ–å®Œæˆ: {vectors.shape}")
        
        return vectors
    
    def build_faiss_index(self, vectors: np.ndarray, index_type: str = "IVF_PQ") -> faiss.Index:
        """æ§‹å»ºFAISSç´¢å¼•"""
        logger.info(f"æ§‹å»ºFAISSç´¢å¼•: {index_type}")
        
        dimension = vectors.shape[1]
        
        if index_type == "IVF_PQ":
            # é«˜å£“ç¸®æ¯”ç´¢å¼•
            nlist = min(1024, vectors.shape[0] // 10)  # å‹•æ…‹èª¿æ•´èšé¡æ•¸é‡
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFPQ(quantizer, dimension, nlist, 96, 8)
            
            # è¨“ç·´ç´¢å¼•
            logger.info("è¨“ç·´FAISSç´¢å¼•...")
            index.train(vectors)
            
        elif index_type == "FLAT":
            # å¹³é¢ç´¢å¼•ï¼ˆç²¾ç¢ºæœç´¢ï¼‰
            index = faiss.IndexFlatIP(dimension)
            
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ç´¢å¼•é¡å‹: {index_type}")
        
        # æ·»åŠ å‘é‡
        logger.info("æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
        index.add(vectors)
        
        logger.info(f"ç´¢å¼•æ§‹å»ºå®Œæˆ: {index.ntotal} å€‹å‘é‡")
        return index
    
    def save_processed_data(self, output_dir: str, documents: List[Dict[str, Any]], 
                          vectors: np.ndarray, index: faiss.Index):
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        index_path = output_path / "theology_vectors.index"
        faiss.write_index(index, str(index_path))
        logger.info(f"FAISSç´¢å¼•å·²ä¿å­˜: {index_path}")
        
        # ä¿å­˜æ–‡æª”æ•¸æ“š
        documents_path = output_path / "documents.pkl"
        with open(documents_path, 'wb') as f:
            pickle.dump(documents, f)
        logger.info(f"æ–‡æª”æ•¸æ“šå·²ä¿å­˜: {documents_path}")
        
        # ä¿å­˜å‘é‡æ•¸æ“šï¼ˆå¯é¸ï¼‰
        vectors_path = output_path / "vectors.npy"
        np.save(vectors_path, vectors)
        logger.info(f"å‘é‡æ•¸æ“šå·²ä¿å­˜: {vectors_path}")
        
        # ä¿å­˜å…ƒæ•¸æ“š
        metadata = {
            "total_documents": len(documents),
            "vector_dimension": vectors.shape[1],
            "index_type": type(index).__name__,
            "model_name": self.model_name,
            "chunk_size": self.chunk_size,
            "overlap": self.overlap,
            "created_at": datetime.now().isoformat(),
            "total_vectors": index.ntotal
        }
        
        metadata_path = output_path / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        logger.info(f"å…ƒæ•¸æ“šå·²ä¿å­˜: {metadata_path}")
        
        # è¼¸å‡ºçµ±è¨ˆä¿¡æ¯
        total_size = sum(f.stat().st_size for f in output_path.iterdir())
        logger.info(f"ç¸½è¼¸å‡ºå¤§å°: {total_size / (1024**2):.2f} MB")
    
    def process_documents(self, input_dir: str, output_dir: str, 
                         index_type: str = "IVF_PQ", batch_size: int = 64):
        """å®Œæ•´çš„æ–‡æª”è™•ç†æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹ç¥å­¸æ–‡æª”è™•ç†æµç¨‹...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.initialize_model()
        
        # è¼‰å…¥æ–‡æª”
        documents = self.load_documents_from_directory(input_dir)
        
        if not documents:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æª”")
        
        # å‘é‡åŒ–
        vectors = self.vectorize_documents(documents, batch_size)
        
        # æ§‹å»ºç´¢å¼•
        index = self.build_faiss_index(vectors, index_type)
        
        # ä¿å­˜æ•¸æ“š
        self.save_processed_data(output_dir, documents, vectors, index)
        
        logger.info("âœ… æ–‡æª”è™•ç†å®Œæˆ!")
        
        # è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
        memory_info = psutil.virtual_memory()
        logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        logger.info(f"   - æ–‡æª”ç‰‡æ®µ: {len(documents)}")
        logger.info(f"   - å‘é‡ç¶­åº¦: {vectors.shape[1]}")
        logger.info(f"   - ç´¢å¼•é¡å‹: {index_type}")
        logger.info(f"   - å…§å­˜ä½¿ç”¨: {memory_info.percent}%")

def main():
    parser = argparse.ArgumentParser(description="ç¥å­¸æ–‡æª”æ•¸æ“šè™•ç†å™¨")
    parser.add_argument("--input_dir", required=True, help="è¼¸å…¥æ–‡æª”ç›®éŒ„")
    parser.add_argument("--output_dir", default="./processed_data", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--model_name", 
                       default="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                       help="ä½¿ç”¨çš„æ¨¡å‹åç¨±")
    parser.add_argument("--index_type", choices=["IVF_PQ", "FLAT"], 
                       default="IVF_PQ", help="FAISSç´¢å¼•é¡å‹")
    parser.add_argument("--batch_size", type=int, default=64, help="å‘é‡åŒ–æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--chunk_size", type=int, default=512, help="æ–‡æª”åˆ†å¡Šå¤§å°")
    parser.add_argument("--overlap", type=int, default=50, help="åˆ†å¡Šé‡ç–Šå¤§å°")
    
    args = parser.parse_args()
    
    # å‰µå»ºè™•ç†å™¨
    processor = TheologyDocumentProcessor(args.model_name)
    processor.chunk_size = args.chunk_size
    processor.overlap = args.overlap
    
    try:
        # åŸ·è¡Œè™•ç†
        processor.process_documents(
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            index_type=args.index_type,
            batch_size=args.batch_size
        )
        
    except Exception as e:
        logger.error(f"è™•ç†å¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    main()