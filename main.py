import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
from pathlib import Path

import faiss
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import openai
from dotenv import load_dotenv
import psutil
from tqdm import tqdm
import pickle

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPIæ‡‰ç”¨
app = FastAPI(
    title="ğŸš€ ç¥å­¸çŸ¥è­˜åº« FAISS å‘é‡æœç´¢API",
    description="é«˜æ€§èƒ½æœ¬åœ°å‘é‡æœç´¢ + OpenAIæ™ºèƒ½å•ç­”",
    version="2.0.0"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAIè¨­å®š
openai.api_key = os.getenv("OPENAI_API_KEY")

# å…¨å±€è®Šæ•¸
vector_model = None
faiss_index = None
document_chunks = None
metadata = None
question_cache = {}

# é…ç½®åƒæ•¸
class Config:
    MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # å¤šèªè¨€æ¨¡å‹
    VECTOR_DIM = 768
    INDEX_TYPE = "IVF_PQ"  # é«˜å£“ç¸®æ¯”ç´¢å¼•
    NLIST = 1024  # èšé¡ä¸­å¿ƒæ•¸é‡
    M = 96  # PQå­å‘é‡æ•¸é‡
    NBITS = 8  # ä½æ•¸
    CHUNK_SIZE = 512  # æ–‡æª”åˆ†å¡Šå¤§å°
    OVERLAP = 50  # é‡ç–Šå­—ç¬¦æ•¸
    TOP_K = 10  # æª¢ç´¢æ–‡æª”æ•¸é‡
    CACHE_SIZE = 2000  # å•é¡Œå¿«å–å¤§å°
    CACHE_DURATION = 24 * 3600  # 24å°æ™‚å¿«å–

# æ•¸æ“šæ¨¡å‹
class SearchRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5
    temperature: Optional[float] = 0.7
    use_cache: Optional[bool] = True

class SearchResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    search_time: float
    total_time: float
    cache_hit: bool
    confidence_score: float

class DocumentChunk(BaseModel):
    id: str
    content: str
    source: str
    metadata: Dict[str, Any]

# æ–‡æª”è™•ç†å·¥å…·
class DocumentProcessor:
    def __init__(self):
        self.chunk_size = Config.CHUNK_SIZE
        self.overlap = Config.OVERLAP
    
    def load_documents_from_directory(self, directory_path: str) -> List[DocumentChunk]:
        """å¾ç›®éŒ„è¼‰å…¥æ–‡æª”"""
        chunks = []
        directory = Path(directory_path)
        
        if not directory.exists():
            logger.error(f"ç›®éŒ„ä¸å­˜åœ¨: {directory_path}")
            return chunks
        
        # æ”¯æ´çš„æ–‡ä»¶æ ¼å¼
        supported_formats = ['.txt', '.md', '.json']
        
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in supported_formats:
                try:
                    content = self.load_file(file_path)
                    file_chunks = self.chunk_document(content, str(file_path))
                    chunks.extend(file_chunks)
                    logger.info(f"è™•ç†æ–‡ä»¶: {file_path}, ç”Ÿæˆ {len(file_chunks)} å€‹ç‰‡æ®µ")
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ä»¶å¤±æ•— {file_path}: {e}")
        
        return chunks
    
    def load_file(self, file_path: Path) -> str:
        """è¼‰å…¥å–®å€‹æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def chunk_document(self, content: str, source: str) -> List[DocumentChunk]:
        """å°‡æ–‡æª”åˆ†å¡Š"""
        chunks = []
        content_length = len(content)
        
        for i in range(0, content_length, self.chunk_size - self.overlap):
            chunk_content = content[i:i + self.chunk_size]
            
            # è·³éå¤ªçŸ­çš„ç‰‡æ®µ
            if len(chunk_content.strip()) < 50:
                continue
            
            chunk_id = hashlib.md5(f"{source}_{i}".encode()).hexdigest()
            
            chunk = DocumentChunk(
                id=chunk_id,
                content=chunk_content.strip(),
                source=source,
                metadata={
                    "start_pos": i,
                    "end_pos": min(i + self.chunk_size, content_length),
                    "chunk_index": len(chunks)
                }
            )
            chunks.append(chunk)
        
        return chunks

# FAISSç´¢å¼•ç®¡ç†å™¨
class FAISSIndexManager:
    def __init__(self):
        self.index = None
        self.is_trained = False
    
    def create_index(self, dimension: int) -> faiss.Index:
        """å‰µå»ºFAISSç´¢å¼•"""
        if Config.INDEX_TYPE == "IVF_PQ":
            # é«˜å£“ç¸®æ¯”ç´¢å¼•
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFPQ(
                quantizer, 
                dimension, 
                Config.NLIST, 
                Config.M, 
                Config.NBITS
            )
            logger.info(f"å‰µå»º IVF_PQ ç´¢å¼•: nlist={Config.NLIST}, m={Config.M}, nbits={Config.NBITS}")
        else:
            # ç°¡å–®å¹³é¢ç´¢å¼•
            index = faiss.IndexFlatIP(dimension)
            logger.info("å‰µå»º Flat ç´¢å¼•")
        
        return index
    
    def train_and_add_vectors(self, vectors: np.ndarray):
        """è¨“ç·´ä¸¦æ·»åŠ å‘é‡"""
        dimension = vectors.shape[1]
        self.index = self.create_index(dimension)
        
        # è¨“ç·´ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(self.index, 'train'):
            logger.info("é–‹å§‹è¨“ç·´FAISSç´¢å¼•...")
            self.index.train(vectors)
            self.is_trained = True
        
        # æ·»åŠ å‘é‡
        logger.info(f"æ·»åŠ  {len(vectors)} å€‹å‘é‡åˆ°ç´¢å¼•...")
        self.index.add(vectors)
        
        logger.info(f"ç´¢å¼•æ§‹å»ºå®Œæˆï¼Œç¸½å‘é‡æ•¸: {self.index.ntotal}")
    
    def search(self, query_vector: np.ndarray, k: int = 10):
        """æœç´¢ç›¸ä¼¼å‘é‡"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–")
        
        scores, indices = self.index.search(query_vector, k)
        return scores[0], indices[0]
    
    def save_index(self, filepath: str):
        """ä¿å­˜ç´¢å¼•"""
        if self.index is not None:
            faiss.write_index(self.index, filepath)
            logger.info(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_index(self, filepath: str):
        """è¼‰å…¥ç´¢å¼•"""
        if os.path.exists(filepath):
            self.index = faiss.read_index(filepath)
            self.is_trained = True
            logger.info(f"ç´¢å¼•å·²è¼‰å…¥: {filepath}")
            return True
        return False

# å‘é‡æœç´¢å¼•æ“
class VectorSearchEngine:
    def __init__(self):
        self.model = None
        self.faiss_manager = FAISSIndexManager()
        self.documents = []
        self.cache = {}
    
    async def initialize(self, documents_path: str = None):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        logger.info("ğŸš€ åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“...")
        
        # è¼‰å…¥æ¨¡å‹
        logger.info(f"è¼‰å…¥æ¨¡å‹: {Config.MODEL_NAME}")
        self.model = SentenceTransformer(Config.MODEL_NAME)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é å»ºç´¢å¼•
        index_path = "theology_vectors.index"
        documents_path_pkl = "documents.pkl"
        
        if os.path.exists(index_path) and os.path.exists(documents_path_pkl):
            # è¼‰å…¥é å»ºç´¢å¼•å’Œæ–‡æª”
            logger.info("è¼‰å…¥é å»ºçš„FAISSç´¢å¼•...")
            self.faiss_manager.load_index(index_path)
            
            with open(documents_path_pkl, 'rb') as f:
                self.documents = pickle.load(f)
            
            logger.info(f"è¼‰å…¥å®Œæˆ: {len(self.documents)} å€‹æ–‡æª”ç‰‡æ®µ")
        
        elif documents_path and os.path.exists(documents_path):
            # æ§‹å»ºæ–°ç´¢å¼•
            await self.build_index_from_documents(documents_path)
        
        else:
            # ä½¿ç”¨ç¤ºä¾‹æ•¸æ“š
            await self.build_sample_index()
        
        logger.info("âœ… å‘é‡æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def build_index_from_documents(self, documents_path: str):
        """å¾æ–‡æª”æ§‹å»ºç´¢å¼•"""
        logger.info(f"å¾æ–‡æª”æ§‹å»ºç´¢å¼•: {documents_path}")
        
        # è™•ç†æ–‡æª”
        processor = DocumentProcessor()
        chunks = processor.load_documents_from_directory(documents_path)
        
        if not chunks:
            logger.warning("æœªæ‰¾åˆ°æ–‡æª”ï¼Œä½¿ç”¨ç¤ºä¾‹æ•¸æ“š")
            await self.build_sample_index()
            return
        
        # å‘é‡åŒ–
        logger.info("é–‹å§‹å‘é‡åŒ–æ–‡æª”...")
        texts = [chunk.content for chunk in chunks]
        
        # åˆ†æ‰¹è™•ç†é¿å…å…§å­˜æº¢å‡º
        batch_size = 100
        all_vectors = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="å‘é‡åŒ–é€²åº¦"):
            batch_texts = texts[i:i + batch_size]
            batch_vectors = self.model.encode(batch_texts, show_progress_bar=False)
            all_vectors.append(batch_vectors)
        
        vectors = np.vstack(all_vectors)
        
        # æ§‹å»ºç´¢å¼•
        self.faiss_manager.train_and_add_vectors(vectors)
        self.documents = chunks
        
        # ä¿å­˜ç´¢å¼•å’Œæ–‡æª”
        self.faiss_manager.save_index("theology_vectors.index")
        with open("documents.pkl", 'wb') as f:
            pickle.dump(self.documents, f)
        
        logger.info(f"ç´¢å¼•æ§‹å»ºå®Œæˆ: {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µ")
    
    async def build_sample_index(self):
        """æ§‹å»ºç¤ºä¾‹ç´¢å¼•ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰"""
        logger.info("æ§‹å»ºç¤ºä¾‹ç¥å­¸çŸ¥è­˜ç´¢å¼•...")
        
        sample_documents = [
            "ä¸‰ä½ä¸€é«”æ˜¯åŸºç£æ•™çš„æ ¸å¿ƒæ•™ç¾©ï¼ŒæŒ‡è–çˆ¶ã€è–å­ã€è–éˆä¸‰å€‹ä½æ ¼åœ¨ä¸€å€‹ç¥è–æœ¬è³ªä¸­çš„çµ±ä¸€ã€‚é€™å€‹æ•™ç¾©åœ¨å°¼è¥¿äºä¿¡ç¶“ä¸­å¾—åˆ°äº†æ˜ç¢ºçš„è¡¨è¿°ã€‚",
            "åŸç½ªæ˜¯æŒ‡äººé¡å› äºç•¶å’Œå¤å¨ƒåœ¨ä¼Šç”¸åœ’ä¸­çš„å¢®è½è€Œæ‰¿å—çš„ç½ªæ€§ã€‚å¥§å¤æ–¯ä¸èªç‚ºåŸç½ªæ˜¯äººé¡æœ¬æ€§çš„è…æ•—ï¼Œå½±éŸ¿äº†äººçš„æ„å¿—å’Œç†æ€§ã€‚",
            "å› ä¿¡ç¨±ç¾©æ˜¯æ–°æ•™æ”¹é©çš„æ ¸å¿ƒæ•™ç¾©ï¼Œç”±é¦¬ä¸Â·è·¯å¾·æå‡ºã€‚å®ƒå¼·èª¿äººä¸æ˜¯å› è¡Œç‚ºç¨±ç¾©ï¼Œè€Œæ˜¯å–®ç´”å› ä¿¡å¿ƒè€Œåœ¨ä¸Šå¸é¢å‰è¢«å®£å‘Šç‚ºç¾©ã€‚",
            "é å®šè«–æ˜¯åŠ çˆ¾æ–‡ç¥å­¸çš„é‡è¦æ¦‚å¿µï¼Œèªç‚ºä¸Šå¸åœ¨å‰µä¸–ä¹‹å‰å°±é å®šäº†èª°å°‡å¾—æ•‘ã€‚é€™å€‹æ•™ç¾©åœ¨æ”¹é©å®—å‚³çµ±ä¸­ä½”æœ‰é‡è¦åœ°ä½ã€‚",
            "è–éˆæ˜¯ä¸‰ä½ä¸€é«”çš„ç¬¬ä¸‰å€‹ä½æ ¼ï¼Œåœ¨äº”æ—¬ç¯€é™è‡¨åˆ°ä½¿å¾’èº«ä¸Šã€‚è–éˆçš„å·¥ä½œåŒ…æ‹¬å…‰ç…§ã€é‡ç”Ÿã€æˆè–å’Œè³œäºˆå±¬éˆæ©è³œã€‚",
            "æ•‘æ©æ˜¯ä¸Šå¸æ‹¯æ•‘äººé¡è„«é›¢ç½ªæƒ¡çš„è¨ˆåŠƒï¼Œé€šéè€¶ç©ŒåŸºç£çš„åå­—æ¶å·¥ä½œå¾—ä»¥å¯¦ç¾ã€‚æ•‘æ©åŒ…æ‹¬ç¨±ç¾©ã€æˆè–å’Œå¾—æ¦®è€€ä¸‰å€‹éšæ®µã€‚",
            "è–ç¶“æ˜¯åŸºç£æ•™ä¿¡ä»°çš„æœ€é«˜æ¬Šå¨ï¼Œè¢«èªç‚ºæ˜¯ä¸Šå¸é»˜ç¤ºçš„è©±èªã€‚è–ç¶“åŒ…æ‹¬èˆŠç´„å’Œæ–°ç´„å…©éƒ¨åˆ†ï¼Œå…±66å·æ›¸ã€‚",
            "æ•™æœƒæ˜¯æ‰€æœ‰ä¿¡å¾’çš„ç¾¤é«”ï¼Œè¢«ç¨±ç‚ºåŸºç£çš„èº«é«”ã€‚æ•™æœƒçš„ä½¿å‘½åŒ…æ‹¬æ•¬æ‹œã€åœ˜å¥‘ã€æ•™å°å’Œå®£æ•™ã€‚",
            "æ´—ç¦®æ˜¯åŸºç£æ•™çš„é‡è¦è–ç¦®ï¼Œè±¡å¾µä¿¡å¾’èˆ‡åŸºç£åŒæ­»åŒå¾©æ´»ã€‚ä¸åŒæ•™æ´¾å°æ´—ç¦®çš„æ–¹å¼å’Œæ„ç¾©æœ‰ä¸åŒçš„ç†è§£ã€‚",
            "è–é¤æ˜¯è€¶ç©Œè¨­ç«‹çš„è–ç¦®ï¼Œè¨˜å¿µä»–çš„æ­»å’Œå¾©æ´»ã€‚é¤…å’Œæ¯è±¡å¾µåŸºç£çš„èº«é«”å’Œè¡€ï¼Œä¿¡å¾’é€šéè–é¤èˆ‡ä¸»è¯åˆã€‚"
        ]
        
        # å‰µå»ºæ–‡æª”ç‰‡æ®µ
        chunks = []
        for i, text in enumerate(sample_documents):
            chunk = DocumentChunk(
                id=f"sample_{i}",
                content=text,
                source=f"sample_document_{i}.txt",
                metadata={"type": "sample", "index": i}
            )
            chunks.append(chunk)
        
        # å‘é‡åŒ–
        vectors = self.model.encode([chunk.content for chunk in chunks])
        
        # æ§‹å»ºç´¢å¼•
        self.faiss_manager.train_and_add_vectors(vectors)
        self.documents = chunks
        
        logger.info(f"ç¤ºä¾‹ç´¢å¼•æ§‹å»ºå®Œæˆ: {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µ")
    
    async def search(self, query: str, top_k: int = 5) -> tuple:
        """åŸ·è¡Œå‘é‡æœç´¢"""
        start_time = datetime.now()
        
        # å‘é‡åŒ–æŸ¥è©¢
        query_vector = self.model.encode([query])
        
        # FAISSæœç´¢
        scores, indices = self.faiss_manager.search(query_vector, top_k * 2)  # å¤šæª¢ç´¢ä¸€äº›å‚™é¸
        
        # ç²å–ç›¸é—œæ–‡æª”
        relevant_docs = []
        for score, idx in zip(scores, indices):
            if idx < len(self.documents) and score > 0.3:  # ç›¸é—œæ€§é–¾å€¼
                doc = self.documents[idx]
                relevant_docs.append({
                    "content": doc.content,
                    "source": doc.source,
                    "score": float(score),
                    "metadata": doc.metadata
                })
        
        search_time = (datetime.now() - start_time).total_seconds()
        
        return relevant_docs[:top_k], search_time

# å•ç­”ç”Ÿæˆå™¨
class AnswerGenerator:
    def __init__(self):
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    async def generate_answer(self, query: str, relevant_docs: List[Dict], temperature: float = 0.7) -> tuple:
        """ç”Ÿæˆç­”æ¡ˆ"""
        start_time = datetime.now()
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ–‡ç» {i+1}ã€‘{doc['content']}"
            for i, doc in enumerate(relevant_docs)
        ])
        
        # æ§‹å»ºæç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¥å­¸åŠ©æ‰‹ã€‚è«‹åŸºæ–¼ä»¥ä¸‹ç¥å­¸æ–‡ç»å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

ç¥å­¸æ–‡ç»ï¼š
{context}

ç”¨æˆ¶å•é¡Œï¼š{query}

è«‹éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
2. åŸºæ–¼æä¾›çš„æ–‡ç»å…§å®¹å›ç­”ï¼Œä¸è¦ç·¨é€ ä¿¡æ¯
3. å¦‚æœæ–‡ç»ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹èª å¯¦èªªæ˜
4. å›ç­”è¦æº–ç¢ºã€ç°¡æ½”ä¸”æœ‰å¹«åŠ©
5. å¯ä»¥é©ç•¶å¼•ç”¨æ–‡ç»å…§å®¹
6. ä¿æŒå®¢è§€å’Œå­¸è¡“æ€§çš„èªèª¿

å›ç­”ï¼š"""

        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=1500
            )
            
            answer = response.choices[0].message.content
            generation_time = (datetime.now() - start_time).total_seconds()
            
            return answer, generation_time
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", 0

# å¿«å–ç®¡ç†å™¨
class CacheManager:
    def __init__(self):
        self.cache = {}
        self.max_size = Config.CACHE_SIZE
        self.duration = Config.CACHE_DURATION
    
    def _get_cache_key(self, query: str) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    def get(self, query: str) -> Optional[Dict]:
        """ç²å–å¿«å–"""
        key = self._get_cache_key(query)
        
        if key in self.cache:
            cached_data = self.cache[key]
            
            # æª¢æŸ¥æ˜¯å¦éæœŸ
            if datetime.now().timestamp() - cached_data['timestamp'] < self.duration:
                logger.info(f"ğŸ¯ å¿«å–å‘½ä¸­: {query[:50]}...")
                return cached_data['data']
            else:
                # æ¸…ç†éæœŸå¿«å–
                del self.cache[key]
        
        return None
    
    def set(self, query: str, data: Dict):
        """è¨­ç½®å¿«å–"""
        key = self._get_cache_key(query)
        
        # æ¸…ç†èˆŠå¿«å–
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
        
        self.cache[key] = {
            'data': data,
            'timestamp': datetime.now().timestamp()
        }
        
        logger.info(f"ğŸ’¾ å¿«å–å·²ä¿å­˜: {query[:50]}... (å¿«å–å¤§å°: {len(self.cache)})")

# å…¨å±€å¯¦ä¾‹
search_engine = VectorSearchEngine()
answer_generator = AnswerGenerator()
cache_manager = CacheManager()

# APIç«¯é»
@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶"""
    logger.info("ğŸš€ å•Ÿå‹•ç¥å­¸çŸ¥è­˜åº« FAISS å‘é‡æœç´¢æœå‹™...")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šé‡
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("âŒ æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šé‡")
        raise ValueError("OPENAI_API_KEY is required")
    
    # åˆå§‹åŒ–æœç´¢å¼•æ“
    documents_path = os.getenv("DOCUMENTS_PATH", "./documents")
    await search_engine.initialize(documents_path)
    
    # è¼¸å‡ºç³»çµ±ä¿¡æ¯
    memory_info = psutil.virtual_memory()
    logger.info(f"ğŸ’¾ ç³»çµ±å…§å­˜: {memory_info.total // (1024**3)}GB (å¯ç”¨: {memory_info.available // (1024**3)}GB)")
    logger.info("âœ… æœå‹™å•Ÿå‹•å®Œæˆ")

@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "ç¥å­¸çŸ¥è­˜åº« FAISS å‘é‡æœç´¢API",
        "version": "2.0.0",
        "status": "running",
        "features": [
            "ğŸš€ é«˜æ€§èƒ½FAISSå‘é‡æœç´¢",
            "ğŸ§  å¤šèªè¨€èªç¾©ç†è§£",
            "ğŸ’¾ æ™ºèƒ½å•é¡Œå¿«å–",
            "ğŸ¯ ç²¾ç¢ºæ–‡æª”æª¢ç´¢",
            "âœ¨ OpenAIæ™ºèƒ½å•ç­”"
        ]
    }

@app.get("/api/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    memory_info = psutil.virtual_memory()
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "memory_usage": f"{memory_info.percent}%",
        "available_memory": f"{memory_info.available // (1024**2)}MB",
        "index_loaded": search_engine.faiss_manager.index is not None,
        "documents_count": len(search_engine.documents),
        "cache_size": len(cache_manager.cache)
    }

@app.post("/api/search", response_model=SearchResponse)
async def vector_search(request: SearchRequest):
    """å‘é‡æœç´¢API"""
    start_time = datetime.now()
    
    try:
        # æª¢æŸ¥å¿«å–
        cache_hit = False
        if request.use_cache:
            cached_result = cache_manager.get(request.question)
            if cached_result:
                cached_result['cache_hit'] = True
                return SearchResponse(**cached_result)
        
        # åŸ·è¡Œå‘é‡æœç´¢
        relevant_docs, search_time = await search_engine.search(
            request.question, 
            request.top_k
        )
        
        if not relevant_docs:
            return SearchResponse(
                answer="æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šä¾†å›ç­”é€™å€‹å•é¡Œã€‚è«‹å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµè©æˆ–æ›´å…·é«”çš„å•é¡Œã€‚",
                sources=[],
                search_time=search_time,
                total_time=(datetime.now() - start_time).total_seconds(),
                cache_hit=False,
                confidence_score=0.0
            )
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer, generation_time = await answer_generator.generate_answer(
            request.question,
            relevant_docs,
            request.temperature
        )
        
        # è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸
        avg_score = sum(doc['score'] for doc in relevant_docs) / len(relevant_docs)
        confidence_score = min(avg_score * 1.2, 1.0)  # èª¿æ•´ä¿¡å¿ƒåˆ†æ•¸
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            "answer": answer,
            "sources": relevant_docs,
            "search_time": search_time,
            "total_time": total_time,
            "cache_hit": cache_hit,
            "confidence_score": confidence_score
        }
        
        # ä¿å­˜åˆ°å¿«å–
        if request.use_cache and confidence_score > 0.5:
            cache_manager.set(request.question, result)
        
        return SearchResponse(**result)
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±æ•—: {str(e)}")

@app.get("/api/stats")
async def get_stats():
    """ç²å–çµ±è¨ˆä¿¡æ¯"""
    return {
        "documents_count": len(search_engine.documents),
        "index_size": search_engine.faiss_manager.index.ntotal if search_engine.faiss_manager.index else 0,
        "cache_size": len(cache_manager.cache),
        "model_name": Config.MODEL_NAME,
        "vector_dimension": Config.VECTOR_DIM,
        "index_type": Config.INDEX_TYPE
    }

@app.post("/api/clear_cache")
async def clear_cache():
    """æ¸…ç†å¿«å–"""
    cache_manager.cache.clear()
    return {"message": "å¿«å–å·²æ¸…ç†", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "faiss_main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        workers=1,
        log_level="info"
    )